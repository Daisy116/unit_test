創python套件的虛擬環境(後一個venv是資料夾名): python3 -m venv venv
在~/chromedriver_linux64/venv進入python的虛擬環境: source bin/activate
之後就可以下指令: pip install xxx
或者打開 jupyter notebook
===================================================================================================
data website
http://live2.gooooal.com/
http://analysis.gooooal.com/matchlist/2019/fb_league_20191005.html
http://live2.gooooal.com/jzspfx.html


https://matthung0807.blogspot.com/2019/05/python-windows-python-3.html
https://www.python.org/downloads/windows/

https://blog.csdn.net/xueyao0201/article/details/79300098

https://matthung0807.blogspot.com/2019/08/windows-pythonpip.html


https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f




https://ithelp.ithome.com.tw/articles/10194149   爬蟲範例，成功
解決問題
https://medium.com/@Epicure1709/%E4%BD%BF%E7%94%A8python%E7%9A%84selenium%E6%99%82%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%95%8F%E9%A1%8C-7fb5de198ff7
https://sites.google.com/a/chromium.org/chromedriver/downloads
http://www.pcschool.tv/cm/support/qaview.aspx?sno=87


python --version
python -m pip --version
python -m pip list
python -m pip install --upgrade pip
pip3 install jupyter
jupyter notebook

pip install selenium
pip install BeautifulSoup4
pip install openpyxl
pip install lxml

python -m pip list



pip install wheel
pip install pandas
pip install lxml
pip install html5lib

pip install requests

https://jerrynest.io/python-pandas-get-data/







https://www.youtube.com/watch?v=MQH4Rau_F_A   教學影片！！！
https://blog.csdn.net/bqw18744018044/article/details/81351137  無頭模式

未看
https://medium.com/datainpoint/python-essentials-parsing-html-5620b4c06e50
透過 requests 請求 HTML 資料!!!!!




python解析json檔
https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/370193/
https://blog.csdn.net/hacker_Lees/article/details/77866338

import urllib.request
import json
html = urllib.request.urlopen(r'http://odds.gooooal.com/match/1538/r_1538196.json')
hjson = json.loads(html.read())
# print(hjson)
print(hjson['al'])




cid_num = ['133', '125', '85', '115', '84', '81', '88', '82', '126', '129', '130', '131', '95', '118', '123', '185', '200', '117', '132', '80', '90', '120', '121', '97', '96', '112', '217', '109', '113', '108', '91', '99', '104', '100', '94', '127', '100000000', '110', '89', '244', '103', '102', '243', '105', '226', '101', '227', '114', '186', '93', '100000020', '107', '150', '149', '151', '152', '161', '158', '156', '153', '146', '154', '197', '189', '190', '164', '223', '212', '163', '169', '170', '193', '201', '216', '211', '247', '229', '191', '208', '218', '207', '166', '3', '147', '140', '233', '138', '139', '142', '77', '45', '40', '33', '63', '64', '69', '67', '65', '27', '29', '28', '30', '251', '261', '264', '274', '265', '263']
print(cid_num)

get_lid_url = []
for i in cid_num:
    get_lid_url.append('http://app.gooooal.com/competition.do?cid=' + i + '&lang=tr')
print(len(get_lid_url))


# 抓lid_name
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from openpyxl import Workbook
import time

chrome_options = Options()
chrome_options.add_argument('--headless')

browser = webdriver.Chrome('./chromedriver',chrome_options=chrome_options)
# browser.get('http://app.gooooal.com/competition.do?lid=18&sid=2019&pid=22&lang=tr')
browser.get('http://app.gooooal.com/competition.do?cid=130&lang=tr')
# time.sleep(5)
soup = BeautifulSoup(browser.page_source)

lid_name = []
lid_name2 = []

list = []
i = 0
for ele in soup.select('#div_league span'):
#     print(ele.text)
    lid_name2.append(ele.text)
        

for ele in soup.select('#div_league a'):
#     print(ele.text)
    lid_name.append(ele.text)
    
for ele in soup.select('.matchNav_league a'):
    href = ele['href']
    soup_result.append(href)
    

print(lid_name)
print(lid_name2)
browser.close()








for i in range(90,108):
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    
    browser = webdriver.Chrome('./chromedriver',chrome_options=chrome_options)
    browser.get(get_lid_url[i])
    soup = BeautifulSoup(browser.page_source)

